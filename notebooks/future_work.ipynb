{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I give some directions for future work. This is focused more in terms of engineering than machine learning / data science, \n",
    "as the notebooks already include the latter.\n",
    "\n",
    "About the TODOs:\n",
    "- I almost didn't add any check for file formats, and there's a pending function to be implemented to check for IMDB directory format\n",
    "- Implement the DL model, making it sklearn-friendly, so that the base class is compatible\n",
    "- Make the `n_jobs` dependent on the number of cores the machine has\n",
    "- It's more efficient to read just the small sample rather than loading all data and then subsampling.\n",
    "- Add tests for all the code\n",
    "\n",
    "About CLI:\n",
    "- Add some verbose param to silent logger by setting level to warning\n",
    "- Better documentation about what the options in the CLI do\n",
    "- Add option to switch between classical and DL model\n",
    "- Allow user to get soft predictions (probability) from CLI\n",
    "- Turn the config file to raw text file, and allow the user to pass a config file to use alternative settings\n",
    "\n",
    "\n",
    "About better packaging\n",
    "- Added `data/sample` to inside the package but should be somewhere else. I'm not sure how to solve this with poetry. In fact, this is an issue in poetry still to be solved https://github.com/python-poetry/poetry/issues/890\n",
    "- Cythonise code\n",
    "- Not an expert on this but there has to be a way to package the dependencies together with the package, so that it can be installed offline\n",
    "- Add wheel to a (potentially company internal) repository, rather than to the Github repo.\n",
    "\n",
    "\n",
    "About production-ready code\n",
    "- Add missing docstrings\n",
    "- Implement black pre-commit hooks\n",
    "- Use python typing\n",
    "- Use a linter e.g. pylint, and isort\n",
    "- Enable continuous integration\n",
    "\n",
    "About scaling the system\n",
    "- For very big data probably would be better to use something like, e.g. Spark, rather than pandas. Not experienced on this, but actually going to https://pages.databricks.com/202001-EU-EV-UnifyDataMLworkshopLondon_04.WaitlistPage.html next month :)\n",
    "- Set up a flask / django server to enable API calls, so that the server doesn't need to run in the same machine\n",
    "- Allows data to be in a database, scaling better than reading from disk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
