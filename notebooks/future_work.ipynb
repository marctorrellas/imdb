{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I give some directions for future work. This is focused more in terms of engineering than machine learning / data science, \n",
    "as the notebooks already include the latter.\n",
    "\n",
    "About the TODOs:\n",
    "- I almost didn't add any check for file formats, and there's a pending function to be implemented to check for IMDB directory format\n",
    "- Implement the DL model, making it sklearn-friendly, so that the base class is compatible\n",
    "- Make the `n_jobs` dependent on the number of cores the machine has\n",
    "- It's more efficient to read just the small sample rather than loading all data and then subsampling.\n",
    "- Add tests for all the code\n",
    "\n",
    "About CLI:\n",
    "- Add some verbose param to silent logger by setting level to warning\n",
    "- Better documentation about what the options in the CLI do\n",
    "- Add option to switch between classical and DL model\n",
    "- Allow user to get soft predictions (probability) from CLI\n",
    "- Turn the config file to raw text file, and allow the user to pass a config file to use alternative settings\n",
    "\n",
    "\n",
    "About better packaging\n",
    "- Added `data/sample` to inside the package but should be somewhere else. I'm not sure how to solve this with poetry. In fact, this is an issue in poetry still to be solved https://github.com/python-poetry/poetry/issues/890\n",
    "- Cythonise code to speed up and obfuscate IP\n",
    "- Not an expert on this but there has to be a way to package the dependencies together with the package, so that it can be installed offline\n",
    "- Add wheel to a (potentially company internal) repository, rather than to the Github repo.\n",
    "\n",
    "\n",
    "About production-ready code\n",
    "- Add missing docstrings\n",
    "- Implement black pre-commit hooks\n",
    "- Use python typing\n",
    "- Use a linter e.g. pylint, and isort\n",
    "- Enable continuous integration\n",
    "\n",
    "About production-ready system / scaling\n",
    "- For very big data probably would be better to use something like, e.g. Spark, rather than pandas. Dask can also be useful. Not very experienced on this, but actually going to https://pages.databricks.com/202001-EU-EV-UnifyDataMLworkshopLondon_04.WaitlistPage.html next month :)\n",
    "- Set up a flask / django server to enable API calls, so that the server doesn't need to run in the same machine\n",
    "- Allow data to be in a database, scaling better than reading from disk\n",
    "- Use something like `MLflow` to version the model and data used to train it\n",
    "- If speed is critical\n",
    "   - Enable parallelised predictions, usually this is easy and we use\n",
    "   - Some models, like SVM, allow descentralised/parallelised training, where not all data needs to be in the same node\n",
    "   - TF-IDF, for example, allows easy parallelisation. So depending on the final ML pipeline we might parallelise (either across cores or different physical machines) some bits to improve speed/memory\n",
    "- If data is continuously arriving, allow the system to periodically auto-retrain with its own outputs, logging metrics. In each of those iterations, one might potentially want the system to send a random sample of the outputs (some non-confident and some confident), just to review by a human everything is going fine.\n",
    "- In a really big system we should create a Docker image, more easily deployed and machine-agnostic. Then, we can use something like Kubernetes for orchestration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
